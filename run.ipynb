{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lirannoc/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/lirannoc/.conda/envs/ltsf6/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# print torch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "# seed\n",
    "parser.add_argument('--seed', type=int, default=2021, help='random seed')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "#parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--root_path', type=str, default='/cs/cs_groups/azencot_group/TSlib/dataset/ETT-small/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# MTLinear\n",
    "parser.add_argument('--cluster_dist', type=float, default=0.5, help='distance threshhold for clustering, d = 1-cos(alpha). 1 returns one cluster, 0 returns n clusters')\n",
    "parser.add_argument('--layer_type', type=str, default=\"DLinear\", help='later type for MTLinear, supports: DLinear, NLinear , Linear, RLinear')\n",
    "parser.add_argument('--multi_early_stopping', type=bool, default=False, help='multi early stopping for each group')\n",
    "parser.add_argument('--use_horizon_penalty', type=int, default=0 ,help='use the horzion dimension for the loss penalty')\n",
    "parser.add_argument('--use_variates_penalty', type=int, default=0 ,help='use the variates dimension for the loss penalty')\n",
    "parser.add_argument('--penalty_param', type=int, default=1 ,help=' penalty parameter, the larger the stronger the penalty')\n",
    "parser.add_argument('--reproduce_original', type=int, default=1, help='reproduce the original results, ensuring the same random, this is not necessary for the model to work')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# example ili\n",
    "args = parser.parse_args(\"\\\n",
    "         --is_training 1 \\\n",
    "         --root_path ./dataset/illness/ \\\n",
    "         --data_path national_illness.csv \\\n",
    "         --model_id ili \\\n",
    "         --model MTLinear \\\n",
    "         --data custom \\\n",
    "         --features M \\\n",
    "         --seq_len 36 \\\n",
    "         --label_len 18 \\\n",
    "         --pred_len 60 \\\n",
    "         --enc_in 7 \\\n",
    "         --dec_in 7 \\\n",
    "         --c_out 7 \\\n",
    "         --des 'Exp' \\\n",
    "         --itr 1 \\\n",
    "         --learning_rate 0.01\\\n",
    "         --layer_type NLinear\\\n",
    "         --cluster_dist 0.5\\\n",
    "         --use_horizon_penalty 1\\\n",
    "         --use_variates_penalty 1\\\n",
    "         --multi_early_stopping 1\\\n",
    "         --penalty_param 2\\\n",
    "         --batch_size 32\".split()) \n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model = \"MTLinear\"\n",
    "args.cluster_dist = 0.134 # 1, 0.5, 0.293, 0.134, (=1-cos(alpha)). 1 for one cluster, 0 for cluster per variate\n",
    "args.penalty_param = 2 # 1,2. 0 for no penalty\n",
    "args.layer_type = \"DLinear\" # DLinear, NLinear, Linear, RLinear\n",
    "args.multi_early_stopping= 1\n",
    "args.train_epochs = 20\n",
    "args.seed = 2021\n",
    "args.use_horizon_penalty = 1\n",
    "args.use_variates_penalty = 1\n",
    "args.pred_len = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(is_training=1, train_only=False, model_id='ili', model='MTLinear', seed=2021, data='custom', root_path='./dataset/illness/', data_path='national_illness.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=36, label_len=18, pred_len=24, individual=False, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, cluster_dist=0.134, layer_type='DLinear', multi_early_stopping=1, use_horizon_penalty=1, use_variates_penalty=1, penalty_param=2, reproduce_original=1, num_workers=10, itr=1, train_epochs=20, batch_size=32, patience=3, learning_rate=0.01, des=\"'Exp'\", loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)\n",
      "Use GPU: cuda:0\n",
      "train 617\n",
      "variate-cluster allocation:\n",
      "[[2, 3, 4], [5, 6], [0, 1]]\n",
      "layers\n",
      "{'0': DLinear(\n",
      "  (seasonal): Linear(in_features=36, out_features=24, bias=True)\n",
      "  (trend): Linear(in_features=36, out_features=24, bias=True)\n",
      "  (moving_avg): moving_avg(\n",
      "    (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "  )\n",
      "  (decompsition): series_decomp(\n",
      "    (moving_avg): moving_avg(\n",
      "      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "    )\n",
      "  )\n",
      "), '1': DLinear(\n",
      "  (seasonal): Linear(in_features=36, out_features=24, bias=True)\n",
      "  (trend): Linear(in_features=36, out_features=24, bias=True)\n",
      "  (moving_avg): moving_avg(\n",
      "    (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "  )\n",
      "  (decompsition): series_decomp(\n",
      "    (moving_avg): moving_avg(\n",
      "      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "    )\n",
      "  )\n",
      "), '2': DLinear(\n",
      "  (seasonal): Linear(in_features=36, out_features=24, bias=True)\n",
      "  (trend): Linear(in_features=36, out_features=24, bias=True)\n",
      "  (moving_avg): moving_avg(\n",
      "    (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "  )\n",
      "  (decompsition): series_decomp(\n",
      "    (moving_avg): moving_avg(\n",
      "      (avg): AvgPool1d(kernel_size=(25,), stride=(1,), padding=(0,))\n",
      "    )\n",
      "  )\n",
      ")}\n",
      "layer names\n",
      "layers.0.seasonal.weight\n",
      "layers.0.seasonal.bias\n",
      "layers.0.trend.weight\n",
      "layers.0.trend.bias\n",
      "layers.1.seasonal.weight\n",
      "layers.1.seasonal.bias\n",
      "layers.1.trend.weight\n",
      "layers.1.trend.bias\n",
      "layers.2.seasonal.weight\n",
      "layers.2.seasonal.bias\n",
      "layers.2.trend.weight\n",
      "layers.2.trend.bias\n",
      "\n",
      ">>>>>>>start training : ili_MTLinear_custom_ftM_sl36_ll18_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_'Exp'_ltDLinear_pp2_cd0.134_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 617\n",
      "val 74\n",
      "test 170\n",
      "Epoch: 1 cost time: 0.38362956047058105\n",
      "inds-[2, 3, 4]-val loss-0.624627034800748\n",
      "inds-[5, 6]-val loss-0.11242704884110329\n",
      "inds-[0, 1]-val loss-0.4067930648258577\n",
      "0.416045904533738\n",
      "Validation loss decreased (inf --> 0.416046).  Saving model ...\n",
      "Epoch: 1, Steps: 19 | Train Loss: 0.7552160 Vali Loss: 0.4160459 Test Loss: 2.8618112\n",
      "Updating learning rate to 0.01\n",
      "Epoch: 2 cost time: 0.2988710403442383\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.6795429512082288\n",
      "EarlyStopping counter for group-0: 1 out of 3\n",
      "inds-[5, 6]-val loss-0.09657543767631675\n",
      "inds-[0, 1]-val loss-0.4418399884792355\n",
      "EarlyStopping counter for group-2: 1 out of 3\n",
      "0.4450656722765416\n",
      "Validation loss decreased (0.416046 --> 0.445066).  Saving model ...\n",
      "Epoch: 2, Steps: 19 | Train Loss: 0.5869574 Vali Loss: 0.4450657 Test Loss: 3.1761010\n",
      "Updating learning rate to 0.005\n",
      "Epoch: 3 cost time: 0.29163265228271484\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.535692644253787\n",
      "inds-[5, 6]-val loss-0.09511279643629678\n",
      "inds-[0, 1]-val loss-0.3700652805273421\n",
      "0.3624905838126627\n",
      "Validation loss decreased (0.445066 --> 0.362491).  Saving model ...\n",
      "Epoch: 3, Steps: 19 | Train Loss: 0.5666829 Vali Loss: 0.3624906 Test Loss: 2.5328670\n",
      "Updating learning rate to 0.0025\n",
      "Epoch: 4 cost time: 0.2910010814666748\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.5523845136372579\n",
      "EarlyStopping counter for group-0: 1 out of 3\n",
      "inds-[5, 6]-val loss-0.0888266901068467\n",
      "inds-[0, 1]-val loss-0.36387100342350703\n",
      "0.36607841828178306\n",
      "Validation loss decreased (0.362491 --> 0.366078).  Saving model ...\n",
      "Epoch: 4, Steps: 19 | Train Loss: 0.5430659 Vali Loss: 0.3660784 Test Loss: 2.6113408\n",
      "Updating learning rate to 0.00125\n",
      "Epoch: 5 cost time: 0.29310107231140137\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.5440341771851914\n",
      "EarlyStopping counter for group-0: 2 out of 3\n",
      "inds-[5, 6]-val loss-0.08780423728361104\n",
      "inds-[0, 1]-val loss-0.35568342986516654\n",
      "0.35986826655044707\n",
      "Validation loss decreased (0.366078 --> 0.359868).  Saving model ...\n",
      "Epoch: 5, Steps: 19 | Train Loss: 0.5422523 Vali Loss: 0.3598683 Test Loss: 2.6878293\n",
      "Updating learning rate to 0.000625\n",
      "Epoch: 6 cost time: 0.2989983558654785\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.55900796467904\n",
      "setting to best:  layers.0.seasonal.weight\n",
      "setting to best:  layers.0.seasonal.bias\n",
      "setting to best:  layers.0.trend.weight\n",
      "setting to best:  layers.0.trend.bias\n",
      "EarlyStopping counter for group-0: 3 out of 3\n",
      "inds-[5, 6]-val loss-0.08998052367436078\n",
      "EarlyStopping counter for group-1: 1 out of 3\n",
      "inds-[0, 1]-val loss-0.35903061664430425\n",
      "EarlyStopping counter for group-2: 1 out of 3\n",
      "0.3678637392392072\n",
      "Validation loss decreased (0.359868 --> 0.367864).  Saving model ...\n",
      "Epoch: 6, Steps: 19 | Train Loss: 0.5383360 Vali Loss: 0.3678637 Test Loss: 2.6808457\n",
      "Updating learning rate to 0.0003125\n",
      "Epoch: 7 cost time: 0.2968616485595703\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.4975280407557471\n",
      "inds-[5, 6]-val loss-0.08221007076402505\n",
      "inds-[0, 1]-val loss-0.3549779907528621\n",
      "0.3381371779001451\n",
      "Validation loss decreased (0.367864 --> 0.338137).  Saving model ...\n",
      "Epoch: 7, Steps: 19 | Train Loss: 0.5399739 Vali Loss: 0.3381372 Test Loss: 2.5184145\n",
      "Updating learning rate to 0.00015625\n",
      "Epoch: 8 cost time: 0.30164480209350586\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.5163357387064025\n",
      "EarlyStopping counter for group-0: 1 out of 3\n",
      "inds-[5, 6]-val loss-0.08864826531983756\n",
      "EarlyStopping counter for group-1: 1 out of 3\n",
      "inds-[0, 1]-val loss-0.367420479732876\n",
      "EarlyStopping counter for group-2: 1 out of 3\n",
      "0.35159210088923354\n",
      "Validation loss decreased (0.338137 --> 0.351592).  Saving model ...\n",
      "Epoch: 8, Steps: 19 | Train Loss: 0.5480896 Vali Loss: 0.3515921 Test Loss: 2.5250669\n",
      "Updating learning rate to 7.8125e-05\n",
      "Epoch: 9 cost time: 0.30037713050842285\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.547407893373424\n",
      "EarlyStopping counter for group-0: 2 out of 3\n",
      "inds-[5, 6]-val loss-0.07864271634025499\n",
      "inds-[0, 1]-val loss-0.3820661176772167\n",
      "EarlyStopping counter for group-2: 2 out of 3\n",
      "0.3662344783078879\n",
      "Validation loss decreased (0.351592 --> 0.366234).  Saving model ...\n",
      "Epoch: 9, Steps: 19 | Train Loss: 0.5454489 Vali Loss: 0.3662345 Test Loss: 2.5339847\n",
      "Updating learning rate to 3.90625e-05\n",
      "Epoch: 10 cost time: 0.3010365962982178\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.5058250755925352\n",
      "setting to best:  layers.0.seasonal.weight\n",
      "setting to best:  layers.0.seasonal.bias\n",
      "setting to best:  layers.0.trend.weight\n",
      "setting to best:  layers.0.trend.bias\n",
      "EarlyStopping counter for group-0: 3 out of 3\n",
      "inds-[5, 6]-val loss-0.08841684477617189\n",
      "EarlyStopping counter for group-1: 1 out of 3\n",
      "inds-[0, 1]-val loss-0.35729257271547493\n",
      "setting to best:  layers.2.seasonal.weight\n",
      "setting to best:  layers.2.seasonal.bias\n",
      "setting to best:  layers.2.trend.weight\n",
      "setting to best:  layers.2.trend.bias\n",
      "EarlyStopping counter for group-2: 3 out of 3\n",
      "0.3441277231086999\n",
      "Validation loss decreased (0.366234 --> 0.344128).  Saving model ...\n",
      "Epoch: 10, Steps: 19 | Train Loss: 0.5423169 Vali Loss: 0.3441277 Test Loss: 2.5373330\n",
      "Updating learning rate to 1.953125e-05\n",
      "Epoch: 11 cost time: 0.3199043273925781\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.5370870582862861\n",
      "setting to best:  layers.0.seasonal.weight\n",
      "setting to best:  layers.0.seasonal.bias\n",
      "setting to best:  layers.0.trend.weight\n",
      "setting to best:  layers.0.trend.bias\n",
      "EarlyStopping counter for group-0: 3 out of 3\n",
      "inds-[5, 6]-val loss-0.08730871519461895\n",
      "EarlyStopping counter for group-1: 2 out of 3\n",
      "inds-[0, 1]-val loss-0.3809984807933991\n",
      "setting to best:  layers.2.seasonal.weight\n",
      "setting to best:  layers.2.seasonal.bias\n",
      "setting to best:  layers.2.trend.weight\n",
      "setting to best:  layers.2.trend.bias\n",
      "EarlyStopping counter for group-2: 3 out of 3\n",
      "0.36398222383355633\n",
      "Validation loss decreased (0.344128 --> 0.363982).  Saving model ...\n",
      "Epoch: 11, Steps: 19 | Train Loss: 0.5437711 Vali Loss: 0.3639823 Test Loss: 2.5251818\n",
      "Updating learning rate to 9.765625e-06\n",
      "Epoch: 12 cost time: 0.2906336784362793\n",
      "loading checkpoint\n",
      "inds-[2, 3, 4]-val loss-0.5081743859013336\n",
      "setting to best:  layers.0.seasonal.weight\n",
      "setting to best:  layers.0.seasonal.bias\n",
      "setting to best:  layers.0.trend.weight\n",
      "setting to best:  layers.0.trend.bias\n",
      "EarlyStopping counter for group-0: 3 out of 3\n",
      "inds-[5, 6]-val loss-0.09195007640907231\n",
      "setting to best:  layers.1.seasonal.weight\n",
      "setting to best:  layers.1.seasonal.bias\n",
      "setting to best:  layers.1.trend.weight\n",
      "setting to best:  layers.1.trend.bias\n",
      "EarlyStopping counter for group-1: 3 out of 3\n",
      "inds-[0, 1]-val loss-0.3656264105423664\n",
      "setting to best:  layers.2.seasonal.weight\n",
      "setting to best:  layers.2.seasonal.bias\n",
      "setting to best:  layers.2.trend.weight\n",
      "setting to best:  layers.2.trend.bias\n",
      "EarlyStopping counter for group-2: 3 out of 3\n",
      "0.3485251616581254\n",
      "Validation loss decreased (0.363982 --> 0.348525).  Saving model ...\n",
      "Epoch: 12, Steps: 19 | Train Loss: 0.5436889 Vali Loss: 0.3485252 Test Loss: 2.5246167\n",
      "Early stopping\n",
      ">>>>>>>testing : ili_MTLinear_custom_ftM_sl36_ll18_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_'Exp'_ltDLinear_pp2_cd0.134_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 170\n",
      "mse:2.5254034996032715, mae:1.0397692918777466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.dvices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_lt{}_pp{}_cd{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des,\n",
    "            args.layer_type,\n",
    "            args.penalty_param,\n",
    "            args.cluster_dist, ii)\n",
    "        \n",
    "\n",
    "        random.seed(args.seed+ii)\n",
    "        torch.manual_seed(args.seed+ii)\n",
    "        np.random.seed(args.seed+ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        if not args.train_only:\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "\n",
    "\n",
    "        if args.do_predict:\n",
    "            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.predict(setting, True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
    "                                                                                                  args.model,\n",
    "                                                                                                  args.data,\n",
    "                                                                                                  args.features,\n",
    "                                                                                                  args.seq_len,\n",
    "                                                                                                  args.label_len,\n",
    "                                                                                                  args.pred_len,\n",
    "                                                                                                  args.d_model,\n",
    "                                                                                                  args.n_heads,\n",
    "                                                                                                  args.e_layers,\n",
    "                                                                                                  args.d_layers,\n",
    "                                                                                                  args.d_ff,\n",
    "                                                                                                  args.factor,\n",
    "                                                                                                  args.embed,\n",
    "                                                                                                  args.distil,\n",
    "                                                                                                  args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "\n",
    "    if args.do_predict:\n",
    "        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.predict(setting, True)\n",
    "    else:\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltsf6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
